{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow==2.16 keras==3.8 matplotlib seaborn scikit-learn notebook pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.api.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.api.preprocessing import image_dataset_from_directory\n",
    "from keras.api.applications import ResNet50\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: Define Dataset Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths to your dataset\n",
    "TRAINING_DIR = '/Users/dianasill/Documents/Projects/PneumoniaXRayML/pneumonia_detection_model/chest_xray/train'\n",
    "VALIDATION_DIR = '/Users/dianasill/Documents/Projects/PneumoniaXRayML/pneumonia_detection_model/chest_xray/val'\n",
    "TEST_DIR = '/Users/dianasill/Documents/Projects/PneumoniaXRayML/pneumonia_detection_model/chest_xray/test'\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = 150  # Resize the images to this size\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3: Load and Preprocess Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset using image_dataset_from_directory\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='binary',\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "val_dataset = image_dataset_from_directory(\n",
    "    VALIDATION_DIR,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='binary',\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    TEST_DIR,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='binary',\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# Configure datasets for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: Define Data Augmentation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomWidth(0.2),\n",
    "    tf.keras.layers.RandomHeight(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    tf.keras.layers.RandomBrightness(0.2),\n",
    "    tf.keras.layers.RandomContrast(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 5: Build and Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning with ResNet50\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "# Freeze the base model layers initially\n",
    "base_model.trainable = False\n",
    "\n",
    "# Model Architecture\n",
    "model = Sequential([\n",
    "    data_augmentation,  # Data Augmentation Layer\n",
    "    base_model,  # Pretrained ResNet50\n",
    "    GlobalAveragePooling2D(),  # Global Average Pooling to reduce dimensions\n",
    "    Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),  # Dense layer with L2 regularization\n",
    "    Dropout(0.5),  # Dropout for regularization\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 6: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping and ReduceLROnPlateau\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 7: Fine-Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model: Unfreeze some layers\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:143]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model for fine-tuning\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model further\n",
    "history_finetune = model.fit(\n",
    "    train_dataset,\n",
    "    # change EPOCHS to something more manageable\n",
    "    epochs=7,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('pneumonia_detection_model_finetuned.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 8: Plot Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 9: Evaluate and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_images(dataset):\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    for images, labels in dataset:\n",
    "        preds = model.predict(images)\n",
    "        true_labels.extend(labels.numpy())\n",
    "        pred_labels.extend((preds > 0.5).astype(int))\n",
    "\n",
    "    return true_labels, pred_labels\n",
    "\n",
    "def plot_confusion_matrix(true_labels, pred_labels):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Pneumonia'], yticklabels=['Normal', 'Pneumonia'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "# Validation Set Results\n",
    "true_labels, pred_labels = predict_all_images(val_dataset)\n",
    "print(\"Validation Set Results:\")\n",
    "print(classification_report(true_labels, pred_labels, target_names=['Normal', 'Pneumonia']))\n",
    "plot_confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# Test Set Results\n",
    "true_labels_test, pred_labels_test = predict_all_images(test_dataset)\n",
    "print(\"Test Set Results:\")\n",
    "print(classification_report(true_labels_test, pred_labels_test, target_names=['Normal', 'Pneumonia']))\n",
    "plot_confusion_matrix(true_labels_test, pred_labels_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
